"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[794],{368:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>a,contentTitle:()=>r,default:()=>h,frontMatter:()=>c,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module-4-vla-capstone/voice-processing","title":"Voice Processing and Speech Recognition","description":"Overview","source":"@site/docs/module-4-vla-capstone/voice-processing.md","sourceDirName":"module-4-vla-capstone","slug":"/module-4-vla-capstone/voice-processing","permalink":"/docs/module-4-vla-capstone/voice-processing","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla-capstone/voice-processing.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to Module 4: Voice-Language-Action (VLA) Pipeline","permalink":"/docs/module-4-vla-capstone/intro"},"next":{"title":"Language Understanding and LLM Integration","permalink":"/docs/module-4-vla-capstone/language-understanding"}}');var o=n(4848),t=n(8453);const c={sidebar_position:2},r="Voice Processing and Speech Recognition",a={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Chapter Outline",id:"chapter-outline",level:2},{value:"Hands-on Activities",id:"hands-on-activities",level:2}];function d(e){const i={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(i.header,{children:(0,o.jsx)(i.h1,{id:"voice-processing-and-speech-recognition",children:"Voice Processing and Speech Recognition"})}),"\n",(0,o.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(i.p,{children:"This chapter introduces the voice processing component of VLA systems, focusing on converting speech to text using advanced models like Whisper and integrating with ROS 2 systems. Voice processing is the first step in creating natural human-robot interaction through spoken commands."}),"\n",(0,o.jsx)(i.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(i.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Implement speech recognition using Whisper"}),"\n",(0,o.jsx)(i.li,{children:"Process audio input for robotic applications"}),"\n",(0,o.jsx)(i.li,{children:"Integrate voice processing with ROS 2 systems"}),"\n",(0,o.jsx)(i.li,{children:"Handle real-time voice streaming and buffering"}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Understanding of ROS 2 concepts from Module 1"}),"\n",(0,o.jsx)(i.li,{children:"Basic knowledge of audio processing concepts"}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"chapter-outline",children:"Chapter Outline"}),"\n",(0,o.jsx)(i.p,{children:"This chapter covers:"}),"\n",(0,o.jsxs)(i.ol,{children:["\n",(0,o.jsx)(i.li,{children:"Speech recognition fundamentals and Whisper architecture"}),"\n",(0,o.jsx)(i.li,{children:"Audio preprocessing and noise reduction"}),"\n",(0,o.jsx)(i.li,{children:"Real-time voice processing considerations"}),"\n",(0,o.jsx)(i.li,{children:"Integration with ROS 2 audio systems"}),"\n",(0,o.jsx)(i.li,{children:"Voice activity detection and streaming"}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"hands-on-activities",children:"Hands-on Activities"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Set up Whisper for speech recognition"}),"\n",(0,o.jsx)(i.li,{children:"Create a ROS 2 node for audio input handling"}),"\n",(0,o.jsx)(i.li,{children:"Implement voice activity detection"}),"\n",(0,o.jsx)(i.li,{children:"Test speech recognition with various audio inputs"}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,o.jsx)(i,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>c,x:()=>r});var s=n(6540);const o={},t=s.createContext(o);function c(e){const i=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:c(e.components),s.createElement(t.Provider,{value:i},e.children)}}}]);